{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fe9c27",
   "metadata": {},
   "source": [
    "# Tech Talent Dashboard Data Preprocessing\n",
    "\n",
    "This notebook steps through the process of automating the cleaning and aggregation of raw data provided by the Bureau of Labor Statistics.  It contains the scripts represented by the **ETL Scripts** step of the architecture diagram below.\n",
    "\n",
    "<img src='architecture_p1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c031fff",
   "metadata": {},
   "source": [
    "# Key Assumptions\n",
    "\n",
    "1. **Google Drive** is used to store the original, unmodified data files collected from various public and proprietary sources.  This location is referred to as `data` or `DATA_DIR` in various places throughout the project.\n",
    "2. **Google Sheets** is used as a psuedo-database where processed data are stored individually within their own files.  This location is referred to as `db` or `DB_DIR` throughout the project.\n",
    "3. **Tableau Public** can connect directly to data residing on Google Drive and/or Sheets.  However, it can only refresh automatically from Google Sheets data sources.  Automatic refreshes occur once a day, or on demand by the dashboard publisher.\n",
    "4. **GitHub.com** hosts all of the python scripts and other necessary files required for this project.  The raw data files **are not** saved within the repository. All raw data files will be automatically downloaded from Google Drive when the scripts are executed. \n",
    "5. An authorized **API Key** must first be obtained from Google and placed into the project's root directory with the name `key.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398994f9",
   "metadata": {},
   "source": [
    "# High-level Overview\n",
    "\n",
    "1. Connect to Google Drive and identify all raw data files contained in the project's `/data/` folders.\n",
    "2. Download all raw data files to the project's local `/data/` folder.\n",
    "3. Load and prepare the SOC code crosswalks.  These files are used to convert all SOC codes to their most recent versions.  Different (and sometimes multiple) crosswalks are required based on the release year of the data.\n",
    "4. Load the MSA lookup list from `/data/lookups/lk_msa.xlsx`.  **This file determines which metros are included in the final dataset and dashboard.  Adding or removing MSAs from this list and re-running the scripts will add/remove them from the final dataset that feeds into the dashboard.**\n",
    "\n",
    "| area  | area_title                               | state | peer_type         |\n",
    "|-------|------------------------------------------|-------|-------------------|\n",
    "| 28940 | Knoxville, TN                            | TN    | National Peer MSA |\n",
    "| 24860 | Greenville-Anderson-Mauldin, SC          | SC    | National Peer MSA |\n",
    "| 46140 | Tulsa, OK                                | OK    | National Peer MSA |\n",
    "| 30780 | Little Rock-North Little Rock-Conway, AR | AR    | National Peer MSA |\n",
    "| 17900 | Columbia, SC                             | SC    | National Peer MSA |\n",
    "| 24660 | Greensboro-High Point, NC                | NC    | National Peer MSA |\n",
    "| 12940 | Baton Rouge, LA                          | LA    | National Peer MSA |\n",
    "| 31140 | Louisville/Jefferson County, KY-IN       | KY    | National Peer MSA |\n",
    "| 13820 | Birmingham-Hoover, AL                    | AL    | Birmingham MSA    |\n",
    "| 12220 | Auburn-Opelika, AL                       | AL    | Alabama Peer MSA  |\n",
    "| 33660 | Mobile, AL                               | AL    | Alabama Peer MSA  |\n",
    "| 33860 | Montgomery, AL                           | AL    | Alabama Peer MSA  |\n",
    "| 46220 | Tuscaloosa, AL                           | AL    | Alabama Peer MSA  |\n",
    "\n",
    "5. Clean each of the BLS and IPEDS files, applying crosswalks and making all necessary changes.\n",
    "6. Exclude all data for metros not included in the list above to work around table size limits.\n",
    "7. Save the resulting final data files into the `db` folder.\n",
    "8. Connect to Google Sheets and overwrite all contents with the new data.\n",
    "\n",
    "*Note: Tableau Public data connections to Google Sheets uses the file's unique id rather than the filename. Uploading a new copy of the file will also generate a new id for that file, which in turn will break the connection between the dashboard and the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897d812",
   "metadata": {},
   "source": [
    "# Raw data files go in `data` folder \n",
    "\n",
    "Raw data files should be placed into their corresponding files within the ```\\data\\``` folder in Google Drive:\n",
    "\n",
    "```\n",
    "Google Drive\n",
    "    ├─mcdc-tech-talent  \n",
    "            ├─data\n",
    "                ├─ bg\n",
    "                ├─ bls_msa\n",
    "                │    ├─ MSA_M20XX_dl.xlsx\n",
    "                ├─ bls_national\n",
    "                │    ├─ national_M20XX_dl.xlsx\n",
    "                ├─ ipeds\n",
    "                │    ├─ C20XX_A.zip\n",
    "                │    ├─ HD20XX.zip\n",
    "                ├─ lookups\n",
    "                     ├─ CIP2020_SOC2018_Crosswalk.xlsx\n",
    "                     ├─ Crosswalk2010to2020.csv\n",
    "                     ├─ lk_awlevel.xlsx\n",
    "                     ├─ lk_msa.xlsx\n",
    "                     ├─ lk_sectors.xlsx\n",
    "                     ├─ oes_2019_hybrid_structure.xlsx\n",
    "\n",
    "```\n",
    "\n",
    "1. `bg` folder is not needed, but can be used to store burning glass exports\n",
    "2. `bls_msa` folder contains Metropolitan XLS files from https://www.bls.gov/oes/#data\n",
    "3. `bls_national` folder contains National XLS files from https://www.bls.gov/oes/#data\n",
    "4. `ipeds` folder contains Completions and Institutional Characteristics files from https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx?goToReportId=7\n",
    "5. `lookups` folder contains user created files prefixed with `lk_` and:\n",
    "    - CIP2020_SOC2018_Crosswalk.xlsx from https://nces.ed.gov/ipeds/cipcode/Files/CIP2020_SOC2018_Crosswalk.xlsx\n",
    "    - Crosswalk2010to2020.csv from https://nces.ed.gov/ipeds/cipcode/resources.aspx?y=56\n",
    "    - oes_2019_hybrid_structure.xlsx from https://www.bls.gov/oes/oes_2019_hybrid_structure.xlsx\n",
    "    \n",
    "\n",
    "# The `db` folder will be automatically populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa86502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19d640",
   "metadata": {},
   "source": [
    "# Google API\n",
    "\n",
    "These initial steps will create a connection object to the [Google Drive API](https://developers.google.com/drive/api/v3/about-sdk) called `service`.  A valid authentication key from Google must be saved as key.json in the project's root directory.  This project uses [service accounts](https://cloud.google.com/iam/docs/understanding-service-accounts) for authentication to both the Google [Drive](https://developers.google.com/drive/api/v3/about-sdk) and [Sheets](https://developers.google.com/sheets/api/reference/rest) APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae71f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_SECRET_FILE = 'key.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive', \n",
    "          'https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file(CLIENT_SECRET_FILE, scopes=SCOPES)\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "DB_DIR = 'db'\n",
    "\n",
    "DB_PATH = Path() / 'db'\n",
    "DATA_PATH = Path() / 'data'\n",
    "\n",
    "service = build('drive', 'v3', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f1bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_service(api_key, api_name, api_version, scope):\n",
    "\n",
    "    creds = service_account.Credentials.from_service_account_file(api_key, scopes=scope)\n",
    "\n",
    "    try:\n",
    "        service = build(api_name, api_version, credentials=creds)\n",
    "        print(api_name, 'service created successfully')\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print('Unable to connect.')\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4fd7e3",
   "metadata": {},
   "source": [
    "# get_file_id\n",
    "\n",
    "The purpose of this function is to ensure that we reference the correct folders by way of a uniquely assigned file ID value rather than the name of the folder, which may not always be unique within the project or even a folder. [Relevant Google Drive API Documentation](https://developers.google.com/drive/api/v3/reference/files)\n",
    "\n",
    "The function will return the ID value of the specified `file_name` as a string object.  Optionally, a `parent_id` value can be specified in order to constrain results to subfolders found within a specific folder.  If multiple folders are found, it will return the first value and print a warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f6ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_id(service, file_name, mime_type=None, parent_id=None):\n",
    "    \"\"\"Return the ID of a Google Drive file\n",
    "\n",
    "    :param service: A Google Drive API service object\n",
    "    :param file_name: A string, the name of the file\n",
    "    :param mime_type: A string, optional MIME type of file to search for\n",
    "    :param parent_id: A string, optional id of a parent folder to search in\n",
    "\n",
    "    :return file_id: A string, file ID of the first found result\n",
    "    \"\"\"\n",
    "\n",
    "    file_id = None\n",
    "\n",
    "    query = \"\"\"name='{}'\n",
    "               and trashed=False\n",
    "               \"\"\".format(file_name)\n",
    "\n",
    "    if parent_id:\n",
    "        query += \"and parents in '{}'\".format(parent_id)\n",
    "\n",
    "    if mime_type:\n",
    "        query += \"and mimeType in '{}'\".format(mime_type)\n",
    "\n",
    "    try:\n",
    "        results = service.files().list(\n",
    "            q=query,\n",
    "            fields='files(name, id)').execute()\n",
    "\n",
    "        if len(results['files']) > 1:\n",
    "            print('Multiple files found, retrieving first from list')\n",
    "\n",
    "        file_id = results['files'][0]['id']\n",
    "\n",
    "    except Exception as e:\n",
    "        print('An error occurred: {}'.format(e))\n",
    "\n",
    "    return file_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09525db6",
   "metadata": {},
   "source": [
    "Using `get_file_id`, we'll first assign the id of the root folder titled ***data*** into a variable called `data_folder_id`.  The ***data*** folder is where all of our unmodified raw data files will be placed.  We'll then search for the folder title `bls` within the data folder by passing `data_folder_id` as the `parent_id`.  \n",
    "\n",
    "This function helps us minimize the risk of pulling files from the wrong folder.  It also helps to ensure that our project can be easily rebuilt from scratch (if necessary), because the id values are generated by Google Drive even if the file or folder names are the same as before.  This way, the user maintaining the system will not have to keep track of confusing id values in the event that folders or files have to be recreated--we'll just programmatically pull the right ones each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df3b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12Z6wn3e4lJKXl9Zcr9-3bYRsbTvLt7GW\n"
     ]
    }
   ],
   "source": [
    "data_folder_id = get_file_id(service, 'data', 'application/vnd.google-apps.folder')\n",
    "download_folder_id = get_file_id(service, 'bls_msa', 'application/vnd.google-apps.folder', parent_id=data_folder_id)\n",
    "\n",
    "print(download_folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051138ae",
   "metadata": {},
   "source": [
    "# download_file\n",
    "\n",
    "The purpose of this function is to download local copies of the raw data files from the remote Google Drive repository.  It is intended to help troubleshooting of unanticipated issues and errors during the ETL steps.  It will skip any files that are already saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579e9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(service, file_id, file_name, download_path):\n",
    "    \"\"\"Downloads a file from Google Drive to project folder\n",
    "\n",
    "    :param service: A Google Drive API service object\n",
    "    :param file_id: A string, file ID of the file to download\n",
    "    :param file_name: A string, name and extension of file to save locally\n",
    "    :param download_path: A pathlib.Path() object, folder to save file in\n",
    "\n",
    "    :return None\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = download_path / file_name\n",
    "\n",
    "    if file_path.is_file():\n",
    "        print('...{} exists locally, skipping...'.format(file_path))\n",
    "    else:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fd=fh, request=request)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print('...Downloading {}... {}'.format(file_path, status.progress() * 100))\n",
    "\n",
    "        fh.seek(0)\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(fh.read())\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a857ec",
   "metadata": {},
   "source": [
    "Below, we'll use the id of the remote ***bls*** folder that we retrieved previously into `download_folder_id`, and use it to generate a list of id and filename pairs into a variable called `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b37fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1chAdHqD_PFbH3pPDnuqukHW-Br22MN5o', 'name': 'MSA_M2014_dl.xlsx'},\n",
       " {'id': '1sRfrNCBvgyszPrymcUdJfATeZY1TUzsE', 'name': 'MSA_M2016_dl.xlsx'},\n",
       " {'id': '1xGRn9V1nYckEB02yxbPZhCAzQYJFf-Wl', 'name': 'MSA_M2015_dl.xlsx'},\n",
       " {'id': '1eyCpKwuZbw3vstD0F4Zwoc9chjKmPH8P', 'name': 'MSA_M2018_dl.xlsx'},\n",
       " {'id': '1pWjWI2em_LSm8LdmxgMIygSJ4m5WoH9e', 'name': 'MSA_M2020_dl.xlsx'},\n",
       " {'id': '1Ew1n5IHlf6B5sG2UNlOo0Zdr3ocj73Bu', 'name': 'MSA_M2017_dl.xlsx'},\n",
       " {'id': '14uMQPu6IneUp0k88vjsNoxzrJfB3sU8a', 'name': 'MSA_M2019_dl.xlsx'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = service.files().list(q=\"parents in '{}'\".format(download_folder_id), fields='files(name, id)').execute()\n",
    "\n",
    "results['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28507559",
   "metadata": {},
   "source": [
    "We can then use `results` list to loop through and download all of the files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d7d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...data\\bls_msa\\MSA_M2014_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2016_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2015_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2018_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2020_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2017_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2019_dl.xlsx exists locally, skipping...\n"
     ]
    }
   ],
   "source": [
    "for f in results['files']:\n",
    "    download_file(service, f['id'], f['name'], DATA_PATH / 'bls_msa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d60ad5",
   "metadata": {},
   "source": [
    "Lastly, we can put these pieces together to loop through and download all files from each of the three data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad34545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading remote files to local project...\n",
      "...data\\bls_national\\national_M2018_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2016_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2019_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2015_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2014_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2020_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_national\\national_M2017_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2014_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2016_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2015_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2018_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2020_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2017_dl.xlsx exists locally, skipping...\n",
      "...data\\bls_msa\\MSA_M2019_dl.xlsx exists locally, skipping...\n",
      "...data\\bg\\burning_glass_aggregate.xlsx exists locally, skipping...\n",
      "...data\\ipeds\\HD2020.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2020_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2017_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2019_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2018_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2016_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2015_A.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2014.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2016.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2019.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2018.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2015.zip exists locally, skipping...\n",
      "...data\\ipeds\\HD2017.zip exists locally, skipping...\n",
      "...data\\ipeds\\C2014_A.zip exists locally, skipping...\n",
      "...data\\lookups\\lk_msa.xlsx exists locally, skipping...\n",
      "...data\\lookups\\lk_awlevel.xlsx exists locally, skipping...\n",
      "...data\\lookups\\oes_2019_hybrid_structure.xlsx exists locally, skipping...\n",
      "...data\\lookups\\Crosswalk2010to2020.csv exists locally, skipping...\n",
      "...data\\lookups\\lk_sectors.xlsx exists locally, skipping...\n",
      "...data\\lookups\\CIP2020_SOC2018_Crosswalk.xlsx exists locally, skipping...\n",
      "...data\\lookups\\lk_occ_2019_groups.xlsx exists locally, skipping...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Downloads all remote data files to local project folder\"\"\"\n",
    "\n",
    "data_folder_id = get_file_id(service,\n",
    "                             file_name=DATA_DIR,\n",
    "                             mime_type='application/vnd.google-apps.folder')\n",
    "\n",
    "sub_folders = ['bls_national', 'bls_msa', 'bg', 'ipeds', 'lookups']\n",
    "\n",
    "print(\"Downloading remote files to local project...\")\n",
    "for folder in sub_folders:\n",
    "    dl_folder_id = get_file_id(service, folder, parent_id=data_folder_id)\n",
    "    results = service.files().list(\n",
    "        q=\"parents in '{}' and trashed=False\".format(dl_folder_id),\n",
    "        fields='files(name, id)').execute()\n",
    "\n",
    "    download_path = DATA_PATH / folder\n",
    "\n",
    "\n",
    "    for file in results['files']:\n",
    "        download_file(service, file['id'], file['name'], download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f456fe8",
   "metadata": {},
   "source": [
    "# load_msa_lookup / load_oes_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f294ad5",
   "metadata": {},
   "source": [
    "Once the files are downloaded, we'll need to go through and process them.  One of the main data integration tasks in this project is to apply appropriate lookup crosswalks to each dataset.  Before we work on our main datasets, we'll first need to prepare our lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcea4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msa_lookup(data_path):\n",
    "    \"\"\" Loads a lookup table for various MSA specific attributes used\n",
    "    throughout the project.\n",
    "\n",
    "    :param data_path: a pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return msa_lookup: a dictionary of dictionaries containing MSA code to\n",
    "        peer type lookup and MSA code to MSA name lookup\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lookup_dir = 'lookups'\n",
    "    msa_file_name = 'lk_msa.xlsx'\n",
    "\n",
    "    lookup_file_path = data_path / lookup_dir / msa_file_name\n",
    "\n",
    "    msa_lookup = {}\n",
    "\n",
    "    try:\n",
    "        lk_msa = pd.read_excel(lookup_file_path)\n",
    "        msa_lookup['peer_type'] = dict(zip(lk_msa['area'], lk_msa['peer_type']))\n",
    "        msa_lookup['area_title'] = dict(zip(lk_msa['area'], lk_msa['area_title']))\n",
    "        print(\"...MSA lookup table loaded.\")\n",
    "        return msa_lookup\n",
    "    except Exception as e:\n",
    "        print(\"MSA lookup table loading failed - {}\".format(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87d6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oes_lookup(data_path):\n",
    "    \"\"\" Loads a crosswalk for SOC codes provided by Bureau of Labor Statistics\n",
    "    https://www.bls.gov/oes/soc_2018.htm\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return soc_lookup: A dictionary containing crosswalks for various years\n",
    "    \"\"\"\n",
    "\n",
    "    lookup_dir = 'lookups'\n",
    "    oes_file_name = 'oes_2019_hybrid_structure.xlsx'\n",
    "\n",
    "    lookup_file_path = data_path / lookup_dir / oes_file_name\n",
    "\n",
    "    try:\n",
    "        # skip first 5 rows that contain notes\n",
    "        df = pd.read_excel(lookup_file_path, sheet_name=0, skiprows=5)\n",
    "\n",
    "        # rename column headers for easier manipulation\n",
    "        df.columns = ['oes_code_2019', 'oes_title_2019',\n",
    "                      'soc_code_2018', 'soc_title_2018',\n",
    "                      'oes_code_2018', 'oes_title_2018',\n",
    "                      'soc_code_2010', 'soc_title_2010', 'notes']\n",
    "\n",
    "        # create SOC2010 to OES2019 for 2015 and 2016\n",
    "        # create OES2018 to OES2019 for unmatched 2015 and 2016, and 2017-2020\n",
    "        soc1019_dict = dict(zip(df['soc_code_2010'], df['oes_code_2019']))\n",
    "        oes1819_dict = dict(zip(df['oes_code_2018'], df['oes_code_2019']))\n",
    "        oes1919_dict = dict(zip(df['oes_code_2019'], df['oes_code_2019']))\n",
    "        oes19_dict = dict(zip(df['oes_code_2019'], df['oes_title_2019']))\n",
    "\n",
    "        soc_lookup = {'1019': soc1019_dict,\n",
    "                      '1819': oes1819_dict,\n",
    "                      '1919': oes1919_dict,\n",
    "                      '19': oes19_dict}\n",
    "        print(\"...SOC lookup tables loaded.\")\n",
    "        return soc_lookup\n",
    "    except Exception as e:\n",
    "        print(\"SOC lookup table loading failed - {}\".format(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a84ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...SOC lookup tables loaded.\n",
      "...MSA lookup table loaded.\n"
     ]
    }
   ],
   "source": [
    "soc_lookup = load_oes_lookup(DATA_PATH)\n",
    "msa_lookup = load_msa_lookup(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7763efc",
   "metadata": {},
   "source": [
    "# Process BLS files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67037817",
   "metadata": {},
   "source": [
    "The following functions perform various cleaning operations on each BLS data set.:\n",
    "- `much_consistency()` - handles minor inconsistencies in raw data discovered through manual review\n",
    "- `map_soc()` - depending on the report year, applies the correct crosswalks to standardize all occupations to the OES 2018 taxonomy\n",
    "- `map_peer_type()` - labels peer MSAs for ease of aggregation in the analysis layer (National Peers, Alabama Peers, Birmingham)\n",
    "- `map_nulls()` - Handles \\*, \\**, and \\# values in the raw BLS data.  For now, we overwrite these values with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db633023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def much_consistency(df):\n",
    "    \"\"\" Helper function to handle known inconsistencies in raw data files\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data set\n",
    "\n",
    "    :return df: A pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Force lower case column headers\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    print(\"...Forced column headers to lower case.\")\n",
    "\n",
    "    # Rename column headers used inconsistently from year to year\n",
    "    df.rename(columns={'occ_group': 'o_group',\n",
    "                       'loc quotient': 'loc_quotient',\n",
    "                       'area_name': 'area_title'}, inplace=True)\n",
    "    print(\"...Standardized column header names.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4efba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_soc(df, report_year, soc_lookup):\n",
    "    \"\"\" Helper function to apply SOC code crosswalks for older BLS OEWS data.\n",
    "    See: https://www.bls.gov/oes/soc_2018.htm\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data set\n",
    "    :param report_year: An integer, release year of data set\n",
    "    :param soc_lookup: A dictionary, crosswalk loaded via load_oes_lookup()\n",
    "\n",
    "    :return df: A pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    soc_mappings = soc_lookup['1919']\n",
    "\n",
    "    if report_year in [2014, 2015, 2016]:\n",
    "        soc_mappings = soc_lookup['1019']\n",
    "    elif report_year in [2017, 2018]:\n",
    "        soc_mappings = soc_lookup['1819']\n",
    "    elif report_year >= 2019:\n",
    "        soc_mappings = soc_lookup['1919']\n",
    "\n",
    "    # Apply SOC code crosswalk\n",
    "    df['oes_code_2019'] = df[df['o_group'] == 'detailed']['occ_code'].map(soc_mappings)\n",
    "\n",
    "    # Map the couple codes that didn't have a 2010 to 2019 conversion separately\n",
    "    df_matched = df[(~df['oes_code_2019'].isnull()) & (df['o_group'] == 'detailed')].copy()\n",
    "    df_missing = df[(df['oes_code_2019'].isnull()) & (df['o_group'] == 'detailed')].copy()\n",
    "    df_missing['oes_code_2019'] = df_missing['occ_code'].map(soc_lookup['1819'])\n",
    "\n",
    "    # Map the total and major rows separately, because these aren't in the federal crosswalk\n",
    "    df_totals = df[df['o_group'] != 'detailed'].copy()\n",
    "    df_totals['oes_code_2019'] = df_totals['occ_code']\n",
    "    df_totals['oes_title_2019'] = df_totals['occ_title']\n",
    "\n",
    "    # Recombine all rows\n",
    "    df_combined = pd.concat([df_matched, df_missing])\n",
    "    df_combined['oes_title_2019'] = df_combined['oes_code_2019'].map(soc_lookup['19'])\n",
    "\n",
    "    df_f = pd.concat([df_combined, df_totals])\n",
    "\n",
    "    print(\"...Applied SOC crosswalks.\")\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9414378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_peer_type(df, msa_lookup):\n",
    "    \"\"\" Helper function to apply peer group mapping to MSA locations\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data set\n",
    "    :param msa_lookup: a dictionary containing MSA code to peer type lookup\n",
    "\n",
    "    :return df: A pandas dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Map MSA areas to peer type categories\n",
    "    df['peer_type'] = df['area'].map(msa_lookup['peer_type'])\n",
    "    df['peer_type'].fillna(\"All Other MSA\", inplace=True)\n",
    "    print(\"...Applied MSA lookups.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf596dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_msa_names(df, msa_lookup):\n",
    "    \"\"\" Helper function to handle known MSA name changes/inconsistencies\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data set\n",
    "    :param msa_lookup: a dictionary containing MSA code to peer type lookup\n",
    "\n",
    "    :return df: A pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df['area_title'] = df['area'].map(msa_lookup['area_title'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7456dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nulls(df):\n",
    "    \"\"\" Helper function to convert missing or unreported values into nulls\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data set\n",
    "\n",
    "    :return df: a pandas dataframe\n",
    "    \"\"\"\n",
    "    null_replace = {'#': np.nan, '*': np.nan, '**': np.nan}\n",
    "    df.replace(null_replace, inplace=True)\n",
    "    print(\"...Replaced missing and unreported values with nulls.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2b258",
   "metadata": {},
   "source": [
    "`clean_bls_msa()` is the main cleaning function for the MSA specific files that loops through each raw data file and performs each of the individual cleaning steps.  Once all steps are performed, it will constrain the resulting dataset to just peer MSAs identified via `map_peer_types()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13340dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bls_msa(data_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return df_merged: A pandas dataframe, cleaned and merged dataframe\n",
    "                       containing all available years of MSA BLS OEWS data\n",
    "    \"\"\"\n",
    "    df_merged = pd.DataFrame()\n",
    "\n",
    "    bls_folder = data_path / 'bls_msa'\n",
    "\n",
    "    soc_lookup = load_oes_lookup(data_path)\n",
    "    msa_lookup = load_msa_lookup(data_path)\n",
    "\n",
    "    for f in bls_folder.glob('**/*.xlsx'):\n",
    "        print('Processing {} ...'.format(f))\n",
    "\n",
    "        with open(f, 'rb') as file:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "        df = much_consistency(df)\n",
    "\n",
    "        report_year = int(re.search(r\"(M)([1-9]\\d{3,})(_)\", str(f))[2])\n",
    "\n",
    "        df = map_soc(df, report_year, soc_lookup)\n",
    "        df = map_peer_type(df, msa_lookup)\n",
    "\n",
    "        df = map_nulls(df)\n",
    "        df['report_year'] = report_year\n",
    "        \n",
    "        df['occ_code'] = df['oes_code_2019']\n",
    "        df['occ_title'] = df['oes_title_2019']\n",
    "        \n",
    "        df['tech_occ'] = df['occ_code'].str.slice(0,3).map({'15-':'Tech'})\n",
    "        df['tech_occ'].fillna(\"Non-Tech\", inplace=True)\n",
    "\n",
    "        df = df[['report_year', 'area', 'area_title', 'peer_type',\n",
    "                 'occ_code', 'occ_title', 'tech_occ', 'o_group',\n",
    "                 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient',\n",
    "                 'h_mean', 'a_mean', 'mean_prse',\n",
    "                 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90',\n",
    "                 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90']\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "        df_merged = pd.concat([df_merged, df])\n",
    "        print('...{} processing complete.'.format(f))\n",
    "\n",
    "    df_merged = df_merged[df_merged['peer_type'] != 'All Other MSA']\n",
    "    #df_merged = df_merged[df_merged['oes_code_2019'].str.contains('15-')]\n",
    "\n",
    "    df_merged = map_msa_names(df_merged, msa_lookup)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0416872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bls_national(data_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return df_merged: A pandas dataframe, cleaned and merged dataframe\n",
    "                       containing all available years of national BLS OEWS data\n",
    "    \"\"\"\n",
    "    df_merged = pd.DataFrame()\n",
    "\n",
    "    bls_folder = data_path / 'bls_national'\n",
    "\n",
    "    soc_lookup = load_oes_lookup(data_path)\n",
    "    ###msa_lookup = load_msa_lookup(data_path)\n",
    "\n",
    "    for f in bls_folder.glob('**/*.xlsx'):\n",
    "        print('Processing {} ...'.format(f))\n",
    "\n",
    "        with open(f, 'rb') as file:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "        df = much_consistency(df)\n",
    "\n",
    "        report_year = int(re.search(r\"(M)([1-9]\\d{3,})(_)\", str(f))[2])\n",
    "\n",
    "        df = map_soc(df, report_year, soc_lookup)\n",
    "        ### df = map_peer_type(df, msa_lookup)\n",
    "\n",
    "        df = map_nulls(df)\n",
    "        df['report_year'] = report_year\n",
    "        \n",
    "        df['occ_code'] = df['oes_code_2019']\n",
    "        df['occ_title'] = df['oes_title_2019']\n",
    "        \n",
    "        df['tech_occ'] = df['occ_code'].str.slice(0,3).map({'15-':'Tech'})\n",
    "        df['tech_occ'].fillna(\"Non-Tech\", inplace=True)\n",
    "\n",
    "        df = df[['report_year', 'occ_code', 'occ_title', 'tech_occ', 'o_group',\n",
    "                 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse',\n",
    "                 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90',\n",
    "                 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90']\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "        df_merged = pd.concat([df_merged, df])\n",
    "        print('...{} processing complete.'.format(f))\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf3cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...SOC lookup tables loaded.\n",
      "...MSA lookup table loaded.\n"
     ]
    }
   ],
   "source": [
    "soc_lookup = load_oes_lookup(DATA_PATH)\n",
    "msa_lookup = load_msa_lookup(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41896b5",
   "metadata": {},
   "source": [
    "The following command will run all cleaning steps and return a dataframe that contains all of o_group = total / major / detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f5fd87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...SOC lookup tables loaded.\n",
      "...MSA lookup table loaded.\n",
      "Processing data\\bls_msa\\MSA_M2014_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2014_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2015_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2015_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2016_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2016_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2017_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2017_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2018_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2018_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2019_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2019_dl.xlsx processing complete.\n",
      "Processing data\\bls_msa\\MSA_M2020_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Applied MSA lookups.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_msa\\MSA_M2020_dl.xlsx processing complete.\n"
     ]
    }
   ],
   "source": [
    "bls_merged_msa = clean_bls_msa(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b687910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...SOC lookup tables loaded.\n",
      "Processing data\\bls_national\\national_M2014_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2014_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2015_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2015_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2016_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2016_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2017_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2017_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2018_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2018_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2019_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2019_dl.xlsx processing complete.\n",
      "Processing data\\bls_national\\national_M2020_dl.xlsx ...\n",
      "...Forced column headers to lower case.\n",
      "...Standardized column header names.\n",
      "...Applied SOC crosswalks.\n",
      "...Replaced missing and unreported values with nulls.\n",
      "...data\\bls_national\\national_M2020_dl.xlsx processing complete.\n"
     ]
    }
   ],
   "source": [
    "bls_merged_national = clean_bls_national(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7ae72",
   "metadata": {},
   "source": [
    "### split_bls_ogroup\n",
    "\n",
    "This function will take the resulting `bls_merged` dataset and split it into three separate files, each containing a slice of o_group = total / major / detailed.  *For this project's requirements,* separating these three files will make it easier to perform the majority of necessary calculations in the analysis and visualization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b01a6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bls_ogroup(df):\n",
    "    \"\"\" Helper function to split the BLS OEWS data into three separate\n",
    "    dataframes each containing a different o_group level.\n",
    "\n",
    "    :param df: A pandas dataframe, BLS OEWS data that has been cleaned\n",
    "\n",
    "    :return bls_total: A pandas dataframe, o_group = total\n",
    "    :return bls_major: A pandas dataframe, o_group = major\n",
    "    :return bls_detailed: A pandas dataframe, o_group = detailed\n",
    "    \"\"\"\n",
    "    bls_total = df[df['o_group'] == 'total']\n",
    "    bls_major = df[df['o_group'] == 'major']\n",
    "    bls_detailed = df[df['o_group'] == 'detailed']\n",
    "\n",
    "    print(\"BLS data split into total, major, detailed.\")\n",
    "    return bls_total, bls_major, bls_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dbceabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS data split into total, major, detailed.\n"
     ]
    }
   ],
   "source": [
    "df_total_msa, df_major_msa, df_detailed_msa = split_bls_ogroup(bls_merged_msa)\n",
    "\n",
    "df_total_msa.to_excel(DB_PATH / 'bls_msa.total.xlsx', index=False)\n",
    "df_major_msa.to_excel(DB_PATH / 'bls_msa.major.xlsx', index=False)\n",
    "df_detailed_msa.to_excel(DB_PATH / 'bls_msa.detailed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a00be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS data split into total, major, detailed.\n"
     ]
    }
   ],
   "source": [
    "df_total_national, df_major_national, df_detailed_national = split_bls_ogroup(bls_merged_national)\n",
    "\n",
    "df_total_national.to_excel(DB_PATH / 'bls_national.total.xlsx', index=False)\n",
    "df_major_national.to_excel(DB_PATH / 'bls_national.major.xlsx', index=False)\n",
    "df_detailed_national.to_excel(DB_PATH / 'bls_national.detailed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4271e38",
   "metadata": {},
   "source": [
    "# IPEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e94d10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipeds_consistency(df):\n",
    "    \"\"\" Helper function to handle known inconsistencies in raw data files\n",
    "\n",
    "    :param df: A pandas dataframe, IPEDS Completions data set\n",
    "\n",
    "    :return df: A pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strip whitespace from columns headers\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    # Force lower case column headers\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    print(\"...Forced column headers to lower case.\")\n",
    "    \n",
    "    df['cipcode'] = df['cipcode'].apply(fix_cip_code)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3c53431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def fix_cip_code(cip):\n",
    "    \"\"\"\n",
    "    Formats a cip code into the full 8 digit xx.xxxxxx format\n",
    "    \n",
    "    :param cip: A string, cip code\n",
    "    \n",
    "    :return: A string, 8 digit cip code in xx.xxxxxx format\n",
    "    \"\"\"\n",
    "    cip = str(cip)\n",
    "    if cip == '99':\n",
    "        return '99'\n",
    "    \n",
    "    left, right = '', ''\n",
    "    s = re.search(r\"^(\\d{1,2})(\\.?)(\\d{1,6})?$\", cip)\n",
    "    try:\n",
    "        left = s[1].zfill(2)\n",
    "        try:\n",
    "            right = s[3].ljust(4, '0')\n",
    "        except:\n",
    "            right = ''.ljust(4, '0')\n",
    "    except:\n",
    "        #print('Invalid CIP code - {}'.format(str(cip)))\n",
    "        return \"\"\n",
    "\n",
    "    return ('.').join([left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "473b90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_features(df):\n",
    "    df['awards_total'] = df['ctotalt']\n",
    "    df['awards_female'] = df['ctotalw']\n",
    "    df['awards_urm_total'] = df['ctotalt'] - df['cwhitt']\n",
    "    df['awards_urm_female'] = df['ctotalw'] - df['cwhitw']\n",
    "    df['awards_blk_total'] = df['cbkaat']\n",
    "    df['awards_blk_female'] = df['cbkaaw']\n",
    "    \n",
    "    df['awards_asian_total'] = df['casiat']\n",
    "    df['awards_asian_female'] = df['casiaw']\n",
    "    \n",
    "    df['awards_amindian_total'] = df['caiant']\n",
    "    df['awards_amindian_female'] = df['caianw']\n",
    "    \n",
    "    df['awards_hisp_total'] = df['chispt']\n",
    "    df['awards_hisp_female'] = df['chispw']\n",
    "    \n",
    "    df['awards_hawaiian_total'] = df['cnhpit']\n",
    "    df['awards_hawaiian_female'] = df['cnhpiw']\n",
    "    \n",
    "    df['awards_white_total'] = df['cwhitt']\n",
    "    df['awards_white_female'] = df['cwhitw']\n",
    "    \n",
    "    df['awards_multi_total'] = df['c2mort']\n",
    "    df['awards_multi_female'] = df['c2morw']\n",
    "    \n",
    "    df['awards_unknown_total'] = df['cunknt']\n",
    "    df['awards_unknown_female'] = df['cunknw']\n",
    "    \n",
    "    df['awards_nralien_total'] = df['cnralt']\n",
    "    df['awards_nralien_female'] = df['cnralw']\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "461cfa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ipeds_sector_lookup(data_path):\n",
    "    \"\"\" Loads a description table IPEDS UNITID Sectors\n",
    "    \n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return ipeds_sector: A dictionary containing sector to title descriptions\n",
    "    \"\"\"\n",
    "    lookup_dir = 'lookups'\n",
    "    file_name = 'lk_sectors.xlsx'\n",
    "    \n",
    "    file_path = data_path / lookup_dir / file_name\n",
    "    \n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    lk_ipeds_sector = dict(zip(df['sector'], df['sector_title']))\n",
    "    return lk_ipeds_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13c3c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_awlevel_lookup(data_path):\n",
    "    \"\"\" Loads a description table IPEDS awlevel\n",
    "    \n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return ipeds_sector: A dictionary containing awlevel to title descriptions\n",
    "    \"\"\"\n",
    "    lookup_dir = 'lookups'\n",
    "    file_name = 'lk_awlevel.xlsx'\n",
    "    \n",
    "    file_path = data_path / lookup_dir / file_name\n",
    "    \n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    lk_awlevel = dict(zip(df['awlevel'], df['award_title']))\n",
    "    return lk_awlevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c26e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cip1020_lookup(data_path):\n",
    "    \"\"\" Loads a crosswalk for CIP2010 to CIP2020\n",
    "    https://nces.ed.gov/ipeds/cipcode/resources.aspx?y=56\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return cip1020_lookup: A dictionary containing crosswalks for various conversions\n",
    "    \"\"\"\n",
    "    \n",
    "    lk_cip1020 = {}\n",
    "    \n",
    "    lookup_dir = 'lookups'\n",
    "    file_name = 'Crosswalk2010to2020.csv'\n",
    "    \n",
    "    file_path = data_path / lookup_dir / file_name\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    df['cipcode2010'] = df['cipcode2010'].str.replace(\"=\", \"\").str.replace('\"', \"\").apply(fix_cip_code)\n",
    "    df['cipcode2020'] = df['cipcode2020'].str.replace(\"=\", \"\").str.replace('\"', \"\").apply(fix_cip_code)\n",
    "    \n",
    "    df = df[df['cipcode2010'] != \"\"].drop_duplicates()\n",
    "    \n",
    "    lk_cip1020['2010_to_2020'] = dict(zip(df['cipcode2010'], df['cipcode2020']))\n",
    "    lk_cip1020['2020_titles'] = dict(zip(df['cipcode2020'], df['ciptitle2020']))\n",
    "    \n",
    "    return lk_cip1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66426769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tech_cip_lookup(data_path):\n",
    "\n",
    "    \"\"\" Loads a crosswalk for CIP to SOC and SOC to CIP published by NCES\n",
    "    https://nces.ed.gov/ipeds/cipcode/resources.aspx?y=56\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return cip_soc_lookup: A dictionary containing crosswalks for various conversions\n",
    "    \"\"\"\n",
    "\n",
    "    lookup_dir = 'lookups'\n",
    "    sipsoc_file_name = 'CIP2020_SOC2018_Crosswalk.xlsx'\n",
    "\n",
    "    lookup_file_path = data_path / lookup_dir / sipsoc_file_name\n",
    "\n",
    "    # CIP to SOC codes\n",
    "    df = pd.read_excel(lookup_file_path, sheet_name='CIP-SOC')\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df['cip2020code'] = df['cip2020code'].apply(fix_cip_code)\n",
    "\n",
    "    tech_cips = list(df[df['soc2018code'].str[:2] == '15']['cip2020code'].unique())\n",
    "    lk_tech_cips = dict(zip(tech_cips, ['Tech' for c in tech_cips]))\n",
    "\n",
    "    print(\"...Tech CIP lookup table loaded.\")\n",
    "    return lk_tech_cips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ca522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def clean_ipeds(data_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data_path: A pathlib.Path() object, location of data file folder\n",
    "\n",
    "    :return df_merged: A pandas dataframe, cleaned and merged dataframe\n",
    "                       containing all available years of BLS OEWS data\n",
    "    \"\"\"\n",
    "    df_c_merged = pd.DataFrame()\n",
    "\n",
    "    ipeds_folder = data_path / 'ipeds'\n",
    "    \n",
    "    msa_lookup = load_msa_lookup(data_path)\n",
    "    cip1020_lookup = load_cip1020_lookup(data_path)\n",
    "    sector_lookup = load_ipeds_sector_lookup(data_path)\n",
    "    tech_cip_lookup = load_tech_cip_lookup(data_path)\n",
    "    awlevel_lookup = load_awlevel_lookup(data_path)\n",
    "\n",
    "    r = r\"(C)([1-9]\\d{3,})(_A)\"\n",
    "    files = [f.name for f in os.scandir(ipeds_folder) if re.search(r, str(f.name))]\n",
    "    \n",
    "    for f in files:\n",
    "        match = re.search(r, f)\n",
    "        name = match[0]\n",
    "        report_year = match[2]\n",
    "\n",
    "        archive = zipfile.ZipFile(ipeds_folder / '{}.zip'.format(name))\n",
    "        \n",
    "        try:\n",
    "            with archive.open('{}_rv.csv'.format(name.lower())) as csv:\n",
    "                df_c = pd.read_csv(csv)\n",
    "        except:\n",
    "            with archive.open('{}.csv'.format(name.lower())) as csv:\n",
    "                df_c = pd.read_csv(csv)\n",
    "                \n",
    "        hd_file = 'HD{}'.format(report_year)\n",
    "        archive = zipfile.ZipFile(ipeds_folder / '{}.zip'.format(hd_file))\n",
    "        try: \n",
    "            with archive.open('{}.csv'.format(hd_file.lower())) as csv:\n",
    "                df_h = pd.read_csv(csv, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print('{} not found - {}'.format(hd_file, e))\n",
    "            \n",
    "        \n",
    "        df_c = ipeds_consistency(df_c)\n",
    "        df_c = simple_features(df_c)\n",
    "\n",
    "        df_c = df_c[df_c['majornum']==1]\n",
    "        \n",
    "        df_h.columns = map(str.lower, df_h.columns)\n",
    "        df_h = df_h[['unitid', 'instnm', 'cbsa', 'sector', 'longitud', 'latitude']]\n",
    "        \n",
    "        df_c['report_year'] = report_year\n",
    "        df_c = pd.merge(df_c, df_h, on='unitid', how='left')\n",
    "        df_c['area_title'] = df_c['cbsa'].map(msa_lookup['area_title'])\n",
    "        df_c['peer_type'] = df_c['cbsa'].map(msa_lookup['peer_type'])\n",
    "\n",
    "        df_c = df_c[df_c['cipcode'] != '99.0000']\n",
    "        \n",
    "        df_c['cipcode'] = df_c['cipcode'].map(cip1020_lookup['2010_to_2020'])\n",
    "        df_c['cipcode_title'] = df_c['cipcode'].map(cip1020_lookup['2020_titles'])\n",
    "        df_c['tech_cip'] = df_c['cipcode'].map(tech_cip_lookup)\n",
    "        df_c['tech_cip'].fillna(\"Non-Tech\", inplace=True)\n",
    "        df_c['sector_title'] = df_c['sector'].map(sector_lookup)\n",
    "        \n",
    "        df_c['award_title'] = df_c['awlevel'].map(awlevel_lookup)\n",
    "        \n",
    "        df_c = df_c[['report_year', 'unitid', 'instnm', 'cbsa', 'area_title', 'peer_type', 'sector', \n",
    "                     'sector_title', 'longitud', 'latitude', 'cipcode', 'cipcode_title', 'tech_cip', \n",
    "                     'awlevel', 'award_title', 'awards_total', 'awards_female', 'awards_urm_total', \n",
    "                     'awards_urm_female', 'awards_blk_total', 'awards_blk_female', 'awards_asian_total',\n",
    "                     'awards_asian_female', 'awards_amindian_total', 'awards_amindian_female',\n",
    "                     'awards_hisp_total', 'awards_hisp_female', 'awards_hawaiian_total', 'awards_hawaiian_female',\n",
    "                     'awards_white_total', 'awards_white_female', 'awards_multi_total', 'awards_multi_female',\n",
    "                     'awards_unknown_total', 'awards_unknown_female', 'awards_nralien_total', 'awards_nralien_female']]\n",
    "        \n",
    "        df_c = df_c[~df_c['area_title'].isnull()]\n",
    "        \n",
    "        df_c_merged = pd.concat([df_c_merged, df_c])\n",
    "\n",
    "    return df_c_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d822bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...MSA lookup table loaded.\n",
      "...Tech CIP lookup table loaded.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n",
      "...Forced column headers to lower case.\n"
     ]
    }
   ],
   "source": [
    "df_ipeds = clean_ipeds(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55bc55db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53443, 37)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ipeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fc91ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipeds.to_excel(DB_PATH / 'ipeds.awards.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b41547",
   "metadata": {},
   "source": [
    "# Awards by SOC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c5a8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "cipsoc = pd.read_excel(DATA_PATH / 'lookups' / 'CIP2020_SOC2018_Crosswalk.xlsx', sheet_name='CIP-SOC',dtype={'CIP2020Code': object})\n",
    "lk_cipsoc = cipsoc.set_index('CIP2020Code')[['SOC2018Code', 'SOC2018Title']]\n",
    "df_soc_awards = df_ipeds.join(lk_cipsoc, on='cipcode')\n",
    "df_soc_awards.rename(columns={'SOC2018Code': 'occ_code', 'SOC2018Title': 'occ_title'}, inplace=True)\n",
    "df_soc_awards['tech_occ'] = df_soc_awards['occ_code'].str.slice(0,3).map({'15-':'Tech'})\n",
    "df_soc_awards['tech_occ'].fillna(\"Non-Tech\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a348bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tech occ codes only due to Google Sheets 50M cell limit\n",
    "df_soc_awards[df_soc_awards['tech_occ']=='Tech'].to_excel(DB_PATH / 'ipeds.tech_soc_awards.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7883ecc",
   "metadata": {},
   "source": [
    "# Load Google Sheets with final data\n",
    "\n",
    "Run the `tech_talent_data_upload_google.ipynb` file for the final data loading step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
